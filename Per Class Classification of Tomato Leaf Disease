# Required libraries
import numpy as np
import cv2
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, f1_score, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# ------------------------------
# Step 1: Preprocessing Images
# ------------------------------
def preprocess_image(img):
    """
    Apply CLAHE and Bilateral filtering
    img: input image (numpy array)
    """
    # Convert to grayscale if needed
    if len(img.shape) == 3:
        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)
    else:
        gray = img.copy()
    
    # Apply CLAHE
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
    cl_img = clahe.apply(gray)
    
    # Bilateral filtering
    filtered_img = cv2.bilateralFilter(cl_img, d=9, sigmaColor=75, sigmaSpace=75)
    
    return filtered_img

# Example: preprocess your images (assuming images in numpy array X_img)
# X_preprocessed = np.array([preprocess_image(img) for img in X_img])

# ------------------------------
# Step 2: Feature scaling and SMOTE
# ------------------------------
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)  # X = fused 2304-dim feature vectors

smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X_scaled, y)

# ------------------------------
# Step 3: Split data (cross-dataset scenario)
# ------------------------------
# If you have a separate test dataset (cross-dataset)
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, stratify=y_resampled, random_state=42)

# ------------------------------
# Step 4: Define classifiers
# ------------------------------
classifiers = {
    'Decision Tree': DecisionTreeClassifier(random_state=42),
    'SVM': SVC(kernel='rbf', probability=True, random_state=42),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss', random_state=42)
}

# ------------------------------
# Step 5: Train, evaluate, compute Macro/Micro F1
# ------------------------------
for name, clf in classifiers.items():
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    
    acc = accuracy_score(y_test, y_pred)
    macro_f1 = f1_score(y_test, y_pred, average='macro')
    micro_f1 = f1_score(y_test, y_pred, average='micro')
    
    print(f'Classifier: {name}')
    print(f'Cross-Dataset Accuracy: {acc:.4f}')
    print(f'Macro F1: {macro_f1:.4f}, Micro F1: {micro_f1:.4f}')
    print('Per-Class F1:')
    print(classification_report(y_test, y_pred))
    print('----------------------------')
    
# ------------------------------
# Step 6: Optional 5-fold cross-validation
# ------------------------------
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
for name, clf in classifiers.items():
    cv_scores = cross_val_score(clf, X_resampled, y_resampled, cv=skf, scoring='f1_macro')
    print(f'{name} 5-Fold Macro F1 CV: {np.mean(cv_scores):.4f} Â± {np.std(cv_scores):.4f}')
